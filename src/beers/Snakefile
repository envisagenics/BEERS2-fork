# BEERS2 pipeline specification

import functools
import json

from beers.configuration import Configuration, Path
from snakemake.utils import update_config

# Parse and validate the configuration
configuration = Configuration(**config)

config.clear()
config.update(configuration.dict())

samples = config['global_config']['samples']
lanes =  config['sequence_pipeline']['flowcell']['lanes_to_use']
seed = config['seed']
full_logs = config['output'].get('full_logs', False)

@functools.cache
def input_molecule_files(sample):
    directory = config["library_prep_pipeline"]["input"].get("directory_path", None)

    if directory is None:
        return []

    molecule_files_directory = Path(f"{directory}/sample{sample}")
    molecule_files = sorted(molecule_files_directory.glob("molecule_*.txt"))

    print(f"Molecule files for sample {sample}:", molecule_files)
    return molecule_files
    

# Count the numbers of packets for each sample
# by source (distribution or molecule packet) and total
num_packets_from_distribution_for_sample = {
    sample: config['library_prep_pipeline']['input']['from_distribution_data'][sample]['num_packets']
        for sample in samples.keys()
}
num_packets_from_molecule_file_for_sample = {
    sample: len(input_molecule_files(sample))
        for sample in samples.keys()
}
num_total_packets_for_sample = {
    sample: num_packets_from_distribution_for_sample[sample] + num_packets_from_molecule_file_for_sample[sample]
        for sample in samples.keys()
}

wildcard_constraints:
    packet_num = "[0-9]+",
    sample = "[0-9]+",
    sequencer_output_filetype = "sam|bam|fastq",

rule all:
    # These are the default files that BEERS2 will create
    # All dependencies of them will be generated as well.
    input:
        expand(
            Path("results/S{sample}_L{lane}.sam"),
            sample = samples.keys(),
            lane = lanes,
        ) if config['output'].get('output_sam', True) else [],
        expand(
            Path("results/S{sample}_L{lane}.bam"),
            sample = samples.keys(),
            lane = lanes,
        ) if config['output'].get('output_bam', False) else [],
        expand(
            Path("results/S{sample}_L{lane}_R1.fastq"),
            sample = samples.keys(),
            lane = lanes,
        ) if config['output'].get('output_fastq', True) else [],

# Perform validation of configuration
# These functions will throw exception if not validated properly
# and print to standard error the reasons
from beers.library_prep.library_prep_pipeline import LibraryPrepPipeline
from beers.sequence.sequence_pipeline import SequencePipeline
LibraryPrepPipeline.validate(config['library_prep_pipeline'], config['global_config'])
SequencePipeline.validate(config['sequence_pipeline'], config['global_config'])

# Compute the log paths for the library prep steps
lib_prep_step_names = [step['step_name'].rsplit('.')[1] for step in config['library_prep_pipeline']['steps']]
lib_prep_logpaths = [Path(f"library_prep_pipeline/sample{{sample}}/logs/{step}/molecule_pkt{{packet_num}}.log") for step in lib_prep_step_names]

rule run_library_prep_packet_from_molecule_file:
    input:
        molecule_file = lambda wildcards: input_molecule_files(wildcards.sample)[int(wildcards.packet_num)]
    output:
        packet_file = Path("library_prep_pipeline/sample{sample}/from_molecule_files/library_prep_pipeline_result_molecule_pkt{packet_num}.txt"),
        output_quant_file = Path("library_prep_pipeline/sample{sample}/from_distribution/library_prep_pipeline_result_molecule_pkt{packet_num}.quant_file"),
        input_quant_file = Path("library_prep_pipeline/sample{sample}/from_distribution/input_quants/library_prep_pipeline_molecule_pkt{packet_num}.quant_file"),
        log_paths = lib_prep_logpaths,
    params:
        config = json.dumps(config['library_prep_pipeline']),
        global_config = json.dumps(config['global_config']),
        seed = seed,
        full_logs = full_logs,
    resources:
        mem_mb = 12_000
    script:
        "scripts/run_library_prep_pipeline.py"

def get_sample_distribution_files(sample):
    ''' input files needed to generate a sample from distribution '''
    directory = Path(config['library_prep_pipeline']['input']['from_distribution_data'][sample]['sample_data_directory'])
    necessary_files = [
        "custom_genome_1.fa",
        "custom_genome_2.fa",
        "custom_genome_indels_1.txt",
        "custom_genome_indels_2.txt",
        "updated_annotation_1.txt",
        "updated_annotation_2.txt",
        "intron_quantifications.txt",
        "gene_quantifications.txt",
        "isoform_psi_value_quantifications.txt",
        "allelic_imbalance_quantifications.txt",
    ]
    return  [Path(f"{directory}/{filename}") for filename in necessary_files]
    
rule run_library_prep_packet_from_distribution:
    input:
        sample_data_files = lambda wildcards: get_sample_distribution_files(wildcards.sample),
    output:
        packet_file = Path("library_prep_pipeline/sample{sample}/from_distribution/library_prep_pipeline_result_molecule_pkt{packet_num}.txt"),
        output_quant_file = Path("library_prep_pipeline/sample{sample}/from_distribution/library_prep_pipeline_result_molecule_pkt{packet_num}.quant_file"),
        input_quant_file = Path("library_prep_pipeline/sample{sample}/from_distribution/input_quants/library_prep_pipeline_molecule_pkt{packet_num}.quant_file"),
        log_paths = lib_prep_logpaths,
    params:
        num_molecules_per_packet = lambda wildcards: config['library_prep_pipeline']['input']['from_distribution_data'][wildcards.sample]['num_molecules_per_packet'],
        config = json.dumps(config['library_prep_pipeline']),
        global_config = json.dumps(config['global_config']),
        seed = seed,
        full_logs = full_logs,
    resources:
        mem_mb = 12_000
    script:
        "scripts/run_library_prep_pipeline.py"

def get_lib_prep_result_packet_path(sample, packet_num):
    from_molecule_files = num_packets_from_molecule_file_for_sample[sample]
    if int(packet_num) < from_molecule_files:
        return Path(f"library_prep_pipeline/sample{sample}/from_molecule_files/library_prep_pipeline_result_molecule_pkt{packet_num}.txt")
    else:
        return Path(f"library_prep_pipeline/sample{sample}/from_distribution/library_prep_pipeline_result_molecule_pkt{packet_num}.txt")

rule create_cluster_packet:
    input:
        packet = lambda wildcards: get_lib_prep_result_packet_path(wildcards.sample, wildcards.packet_num)
    output:
        Path("sequence_pipeline/sample{sample}/input_cluster_packets/cluster_packet_start_pkt{packet_num}.gzip"),
    params:
        configuration = json.dumps(config['sequence_pipeline']['flowcell']),
        seed = seed,
    script:
        "scripts/create_cluster_packets.py"

# Compute the log paths for the sequencing pipeline
seq_step_names = [step['step_name'].rsplit('.')[1] for step in config['sequence_pipeline']['steps']]
seq_logpaths = [Path(f"sequence_pipeline/sample{{sample}}/logs/{step}/sequence_pipeline_cluster_pkt{{packet_num}}.log") for step in seq_step_names]

rule sequence_cluster_packet:
    input:
        cluster_packet = Path("sequence_pipeline/sample{sample}/input_cluster_packets/cluster_packet_start_pkt{packet_num}.gzip")
    output:
        cluster_packet = Path("sequence_pipeline/sample{sample}/output_cluster_packets/sequence_cluster_packet{packet_num}.gzip"),
        log_paths = seq_logpaths,
    params:
        seed = seed,
        config = json.dumps(config['sequence_pipeline']),
        global_config = json.dumps(config['global_config']),
        full_logs = full_logs,
    resources:
        mem_mb = 12_000
    script:
        'scripts/run_sequence_pipeline.py'

rule create_sequencer_outputs_sam_or_bam:
    input:
        cluster_packets = lambda wildcards: [Path(f"sequence_pipeline/sample{{sample}}/output_cluster_packets/sequence_cluster_packet{packet_num}.gzip")
                                                for packet_num in range(num_total_packets_for_sample[wildcards.sample])],
        reference_genome = config['global_config']['resources']['reference_genome_fasta'],
    output:
        sam_file_paths = expand(Path("results/S{{sample}}_L{lane}.{{sequencer_output_filetype}}"), lane=lanes),
        # For demultiplexing failures:
        bad_file_paths = expand(Path("results/S{{sample}}_unidentifiedL{lane}.{{sequencer_output_filetype}}"), lane=lanes),
    resources:
        mem_mb = 6_000
    script:
        'scripts/create_sequencer_outputs.py'

rule create_sequencer_outputs_fastq:
    input:
        cluster_packets = lambda wildcards: [Path(f"sequence_pipeline/sample{{sample}}/output_cluster_packets/sequence_cluster_packet{packet_num}.gzip")
                                                for packet_num in range(num_total_packets_for_sample[wildcards.sample])],
        reference_genome = config['global_config']['resources']['reference_genome_fasta'],
    output:
        # NOTE: fastq outputs have both R1 and R2 files to create
        output_file_paths_R1 = expand(Path("results/S{{sample}}_L{lane}_R1.{{sequencer_output_filetype}}"), lane=lanes),
        output_file_paths_R2 = expand(Path("results/S{{sample}}_L{lane}_R2.{{sequencer_output_filetype}}"), lane=lanes),
        # For demultiplexing failures:
        bad_file_paths_R1 = expand(Path("results/S{{sample}}_unidentified_L{lane}_R1.{{sequencer_output_filetype}}"), lane=lanes),
        bad_file_paths_R2 = expand(Path("results/S{{sample}}_unidentified_L{lane}_R2.{{sequencer_output_filetype}}"), lane=lanes),
    resources:
        mem_mb = 6_000
    script:
        'scripts/create_sequencer_outputs.py'
