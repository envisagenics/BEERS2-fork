import argparse
import importlib
import pathlib
import os
import time
import numpy as np
import resource
import json
import re
from beers_utils.constants import CONSTANTS
from beers.cluster_packet import ClusterPacket
from beers_utils.general_utils import GeneralUtils
from beers.auditor import Auditor

from beers.abstract_beers_pipeline import AbstractBeersPipeline

class SequencePipeline(AbstractBeersPipeline):
    """
    The class runs all the steps in the sequence pipeline as described and wired together in the configuration
    file.  The point of entry into this class is the static method main().
    """

    stage_name = "sequence_pipeline"
    package = "beers.sequence"

    def __init__(self, output_directory_path):
        """
        Many initialization steps here include identifying the log and data directories and subdirectories so
        that data and log files generated are placed in the correct locations.  Note that the directory structure
        has already been created by the controller.  The steps described in the configuration dictionary are
        instantiated and those instantialed steps are added to a list to further use.
        :param output_directory_path: top level directory for data and logs generated by this pipeline stage
        """
        self.log_directory_path = os.path.join(output_directory_path, CONSTANTS.LOG_DIRECTORY_NAME)
        self.data_directory_path = os.path.join(output_directory_path, CONSTANTS.DATA_DIRECTORY_NAME)

        """
        Moving cluster handling to execute method. This means BEERS2 will instantiate a
        single SequencePipeline object which contains all of the configuration common
        to all potential pipelines (executed on different cluster packets). This is to make
        BEERS2 conform to the same job handling protocol as BEERS_UTILS and CAMPAREE. It
        does break with the OO convention of making an instance of the sequencing pipeline
        for each cluster packet. This change may be temporary, and we determine it's better
        to update BEERS_UTILS and CAMPAREE to more closely follow an object-oriented design.

        Keeping the original code here, for now, so it's easier to revert the change if we
        decide to do so. We can remove this comment once we're happy with the implementation.

        self.cluster_packet = cluster_packet
        subdirectory_list = \
            GeneralUtils.get_output_subdirectories(self.cluster_packet.cluster_packet_id, directory_structure)
        data_subdirectory_path = os.path.join(data_directory_path, *subdirectory_list)
        self.log_file_path = os.path.join(log_directory_path,
                                          f"{SequencePipeline.stage_name}_"
                                          f"cluster_pkt{self.cluster_packet.cluster_packet_id}.log")
        self.global_config = global_config

        # Load and instantiate all steps listed in configuration
        self.steps = []
        for step in configuration['steps']:
            module_name, step_name = step["step_name"].rsplit(".")
            step_log_filename = f"{step_name}_cluster_pkt{self.cluster_packet.cluster_packet_id}.log"
            step_log_file_path = os.path.join(log_directory_path, step_name, *subdirectory_list, step_log_filename)
            parameters = step["parameters"]
            module = importlib.import_module(f'.{module_name}', package=SequencePipeline.package)
            step_class = getattr(module, step_name)
            self.steps.append(step_class(step_log_file_path, parameters, self.global_config))

        results_filename = f"{SequencePipeline.stage_name}_" \
                           f"result_cluster_pkt{self.cluster_packet.cluster_packet_id}.gzip"
        self.results_file_path = os.path.join(data_subdirectory_path, results_filename)
        """

    @staticmethod
    def validate(configuration, global_configuration):
        """
        Static method to run each step validate process to identify errant parameters.  If any errors are found,
        a validation exception is raised.
        """
        steps = []
        # Find the step classes in the configuration
        for step in configuration['steps']:
            module_name, step_name = step["step_name"].rsplit(".")
            parameters = step["parameters"]
            module = importlib.import_module(f'.{module_name}', package=SequencePipeline.package)
            step_class = getattr(module, step_name)
            steps.append(step_class(None, parameters, global_configuration))
        # Validate configuration of each step
        if not all([step.validate() for step in steps]):
            raise BeersSequenceValidationException("Validation error in step: see stderr for details.")

    def execute(self, configuration, global_config, directory_structure, input_cluster_packet):
        """
        Opens the pipeline log for writing and serially runs the execute method of each step object found in the
        step list generated when this pipeline stage was initialized.  The final product (a cluster packet
        modified with additional information) is serialized into a data file.
        :param configuration: dictionary of the configuration data relevant to this pipeline stage.
        :param global_config:  dictionary of the full configuration data
        :param directory_structure: instructions for navigating the subdirectories under the output directory
        :param cluster_packet: the cluster packet to run through this pipeline stage
        :return:
        """

        subdirectory_list = \
            GeneralUtils.get_output_subdirectories(input_cluster_packet.cluster_packet_id, directory_structure)
        data_subdirectory_path = os.path.join(self.data_directory_path, *subdirectory_list)
        log_file_path = os.path.join(self.log_directory_path,
                                     f"{SequencePipeline.stage_name}_"
                                     f"cluster_pkt{input_cluster_packet.cluster_packet_id}.log")
        results_filename = f"{SequencePipeline.stage_name}_" \
                           f"result_cluster_pkt{input_cluster_packet.cluster_packet_id}.gzip"
        results_file_path = os.path.join(data_subdirectory_path, results_filename)

        # Load and instantiate all steps listed in configuration prior to executing them below.
        self.steps = []
        for step in configuration['steps']:
            module_name, step_name = step["step_name"].rsplit(".")
            step_log_filename = f"{step_name}_cluster_pkt{input_cluster_packet.cluster_packet_id}.log"
            step_log_file_path = os.path.join(self.log_directory_path, step_name, *subdirectory_list, step_log_filename)
            parameters = step["parameters"]
            module = importlib.import_module(f'.{module_name}', package=SequencePipeline.package)
            step_class = getattr(module, step_name)
            self.steps.append(step_class(step_log_file_path, parameters, global_config))

        print(f"Execution of the {SequencePipeline.stage_name} Started...")
        with open(log_file_path, 'w') as log_file:
            pipeline_start = time.time()
            cluster_packet = input_cluster_packet
            for step in self.steps:
                cluster_packet = step.execute(cluster_packet)

                # Saves the state of the random number generator after completion of each step in the event that
                # we may need to do to partial repeat of the sequence pipeline stage (although exactly how
                # to implement such a partial repeat hasn't been given serious consideration as yet).
                random_state = np.random.get_state()
                log_file.write(f"# random state following {step.__class__.__name__} is {random_state}\n")
                print(f"{step.__class__.name} complete - process RAM currently at"
                      f" {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1E6} GB")

            pipeline_elapsed_time = time.time() - pipeline_start
            print(f"Finished sequence pipeline in {pipeline_elapsed_time:.1f} seconds")

            # Write final sample to a gzip file for inspection
            cluster_packet.serialize(results_file_path)
            print(f"Output final sample to {results_file_path}")
            log_file.write("Sequencing pipeline completed successfully\n")

    def get_commandline_call(self, configuration, global_config, directory_structure, input_cluster_packet):
        """
        Prepare command to execute the SequencePipeline from the command line,
        given all of the arguments used to run the execute() function.

        NOTE: This function currently does nothing and is included to satisfy the requirements
              of the AbstractBeersPipeline interface, which this class implements. At the
              moment, the command line call for this pipeline is assembled by the dispatcher,
              without any input from the class itself. This code is an intermediate step along
              the way to updating BEERS2 to fully support the BEERS_UTILS job submission
              framework. At the moment, the main function transforms a bunch of the command
              line arguments into objects, before passing them to the execute function. This
              makes it impossible to retrieve the original text of the command line arguments
              from these objects.

        """

        # Retrieve path to the sequence_pipeline_path.py script.
        sequence_pipeline_path = os.path.realpath(__file__)
        # If the above command returns a string with a "pyc" extension, instead
        # of "py", strip off "c" so it points to this script.
        sequence_pipeline_path = sequence_pipeline_path.rstrip('c')

        command = (f" python {sequence_pipeline_path}")
        return command

    def get_validation_attributes(self, configuration, global_config, directory_structure, input_cluster_packet):
        """
        Prepare attributes required by is_output_valid() function to validate
        output generated by the SequencePipeline job.

        NOTE: This method is included to satisfy the require requirements of the
              AbstractBeersPipeline interface, which this class implements. The output from
              this method is currently incompatible with the is_output_valid() method, and
              the validation attributes are assembled by the dispatcher, without any input
              from the class itself. This code is an intermediate step along the way to
              updating BEERS2 to fully support the BEERS_UTILS job submission framework. At
              the moment, the dispatcher uses a generic command to submit the library prep
              and sequencing pipelines, without instantiating objects for either pipeline.
              This means there's not currently any way to call this method from the
              dispatcher, since it's not a static method.
        NOTE: The is_output_valid() method currently requires the output_directory_path,
              which it uses to construct paths to the log and data directories. The log
              and data directories are currently stored as instance variables and would
              require a regex/strip to re-derive the output_directory_path.

        :param configuration: dictionary of the configuration data relevant to this pipeline stage.
                              [Note: this parameter is captured just so get_validation_attributes()
                              accepts the same arguments as get_commandline_call(). It is not used
                              here.]
        :param global_config: dictionary of full config data [Note: this parameter is captured just
                              so get_validation_attributes() accepts the same arguments as
                              get_commandline_call(). It is not used here.]
        :param directory_structure: instructions for navigating the subdirectories under the output
                                    directory
        :param input_cluster_packet: the cluster packet to run through this pipeline stage. Only
                                     the cluster ID is used.
        :return: Dictionary with a SequencePipeline's output_directory_path, cluster_packet_id,
                 and directory_structure.

        """

        validation_attributes = {}
        validation_attributes['data_directory_path'] = self.data_directory_path
        validation_attributes['log_directory_path'] = self.log_directory_path
        validation_attributes['packet_id'] = input_cluster_packet.cluster_packet_id
        validation_attributes['directory_structure'] = directory_structure
        return validation_attributes

    @staticmethod
    def is_output_valid(validation_attributes):
        """
        Check if output of SequencePipeline for a specific job/execution is
        correctly formed and valid, given a job's output directory, molecule,
        packet ID, and directpry structure. Prepare these attributes for a given
        job using the get_validation_attributes() method.

        NOTE: This method is included to satisfy the require requirements of the
              AbstractBeersPipeline interface, which this class implements. This method is
              is currently incompatible with output from the get_validation_attributes()
              method. The validation attributes are assembled by the dispatcher, without any
              input from the class itself. This code is an intermediate step along the way
              to updating BEERS2 to fully support the BEERS_UTILS job submission framework.
              At the moment, the dispatcher uses a generic command to submit the library
              prep and sequencing pipelines, without instantiating objects for either
              pipeline. This means there's not currently any way to call the
              get_validation_attributes() method from the dispatcher, since it's not a
              static method.
        NOTE: The get_validation_attributes() method currently provides separate paths
              for the log and data directories, since it stores both of those paths as
              instance variables. This method derives both of these paths given the
              output_directory_path.

        Parameters
        ----------
        validation_attributes : dict
            A job's output_directory_path, cluster_packet_id, and directory_structure
            when running the sequencing pipeline.

        Returns
        -------
        boolean
            True  - SequencePipeline output files were created and are well formed.
            False - SequencePipeline output files do not exist or are missing data.

        """

        output_directory_path = validation_attributes['output_directory_path']
        cluster_packet_id = validation_attributes['packet_id']
        directory_structure = validation_attributes['directory_structure']

        valid_output = False

        # Construct output filenames/paths
        log_directory_path = os.path.join(output_directory_path, CONSTANTS.LOG_DIRECTORY_NAME)
        data_directory_path = os.path.join(output_directory_path, CONSTANTS.DATA_DIRECTORY_NAME)
        subdirectory_list = \
            GeneralUtils.get_output_subdirectories(cluster_packet_id, directory_structure)
        data_subdirectory_path = os.path.join(data_directory_path, *subdirectory_list)
        log_file_path = os.path.join(log_directory_path,
                                     f"{SequencePipeline.stage_name}_"
                                     f"cluster_pkt{cluster_packet_id}.log")
        results_filename = f"{SequencePipeline.stage_name}_" \
                           f"result_cluster_pkt{cluster_packet_id}.gzip"
        results_file_path = os.path.join(data_subdirectory_path, results_filename)

        if os.path.isfile(results_file_path) and \
           os.path.isfile(log_file_path):

            #Read last line in log file
            line = ""
            with open(log_file_path, "r") as log_file:
                for line in log_file:
                    line = line.rstrip()
            if line == "Sequencing pipeline completed successfully":
                valid_output = True

        return valid_output

    @staticmethod
    def main(seed, configuration, configuration_file_path, output_directory_path,
             directory_structure, cluster_packet_path):
        """
        This method would be called by a command line script in the bin directory.  It sets a random seed, loads a
        directory containing the relevant parts of the user's configuration file, unmarshalls a cluster packet from
        the provided cluster packet filename, initializes and validates the sequence pipeline stage and then
        executes it for the cluster packet.  Since the controller cannot conclude until all sequence pipelines are
        run, the last action taken by the sequence pipeline is to note its completion to the auditor, which records
        the cluster id in an audit file.  This happens regardless of the outcome of this pipeline stage.
        :param seed: value to use as the seed for the random number generator
        :param configuration: the json string containing the configration data specific to the library prep pipeline
        :param configuration_file_path: path to the full configuration data
        :param output_directory_path: top level output directory path for this pipeline stage
        :param directory_structure: instructions for creating the scaffolding needed to house the pipeline data and logs
        :param cluster_packet_path: the file from which to unmarshall the cluster packet
        """
        # Normally the cluster_packet_id should be derived from the serialized cluster_packet in the file.  But if
        # the file cannot be found, we still need an id to report back to the auditor if at all possible.  So we
        # extract it from the file name just in case.
        cluster_packet = None
        cluster_packet_filename = str(pathlib.Path(cluster_packet_path).name)
        cluster_packet_id_pattern = re.compile(r'^.*cluster_packet.*_pkt(\d+)\..*$')
        cluster_packet_id_match = re.match(cluster_packet_id_pattern, cluster_packet_path)
        cluster_packet_id = None if not cluster_packet_id_match else cluster_packet_id_match.group(1)
        try:
            np.random.seed(int(seed))
            configuration = json.loads(configuration)
            with open(configuration_file_path) as config_file:
                global_config = json.load(config_file)
            cluster_packet = ClusterPacket.get_serialized_cluster_packet(cluster_packet_path)
            sequence_pipeline = SequencePipeline(output_directory_path)
            sequence_pipeline.execute(configuration, global_config, directory_structure, cluster_packet)
        finally:
            # Using the file name to recover the cluster packet id only if the cluster_packet was never created.
            # They should be the same.
            cluster_packet_id = cluster_packet_id if not cluster_packet else cluster_packet, cluster_packet_id
            Auditor.note_packet_completed(cluster_packet_id, output_directory_path)


class BeersSequenceValidationException(Exception):
    pass

if __name__ == '__main__':
    """
    Prepare and process command line arguments. This code is an intermediate
    step along to way to updating BEERS2 to full support the BEERS_UTILS job
    submission framework. Ideally, we can move this argument processing to
    the main function itself, and distribute some of the main() function's
    operations to the initialization and execute functions.
    """

    parser = argparse.ArgumentParser(description='Sequence Pipeline')
    parser.add_argument('-s', '--seed', required=True, help="Seed")
    parser.add_argument('-c', '--config', required=True, help='Configuration')
    parser.add_argument('-C', '--config_file', required=True, help='Configuration File Path')
    parser.add_argument('-o', '--output_directory', required=True, help='Path to output directory.')
    parser.add_argument('-p', '--cluster_packet_path', required=True, help="Serialized cluster packet file path.")
    parser.add_argument('-d', '--directory_structure', required=True, help="Structure of data and logs directories.")

    args = parser.parse_args()
    SequencePipeline.main(args.seed,
                          args.config,
                          args.config_file,
                          args.output_directory,
                          args.directory_structure,
                          args.cluster_packet_path)
